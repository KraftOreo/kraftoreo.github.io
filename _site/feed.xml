<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-09-12T20:19:41-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Duo Zhang</title><subtitle>Rutgers University</subtitle><author><name>Duo Zhang</name></author><entry><title type="html">Monte Carlo Tree Search</title><link href="http://localhost:4000/Monte_Carlo_Tree_Search/" rel="alternate" type="text/html" title="Monte Carlo Tree Search" /><published>2023-09-12T00:00:00-04:00</published><updated>2023-09-12T00:00:00-04:00</updated><id>http://localhost:4000/Monte_Carlo_Tree_Search</id><content type="html" xml:base="http://localhost:4000/Monte_Carlo_Tree_Search/"><![CDATA[<h1 id="motivation-of-monte-carlo-tree-search-mcts">Motivation of Monte Carlo Tree Search (MCTS)</h1>
<ul>
  <li>Play games optimally</li>
  <li>To be able to play in a reasonalbe time</li>
</ul>

<h1 id="type-of-games">Type of games</h1>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Deterministic</th>
      <th>Nondeterministic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Fully Observable</td>
      <td>Chess, Checkers, Go</td>
      <td>Backgammon, Monopoly</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>Partially Observable</td>
      <td>BattleShip</td>
      <td>Card Games</td>
    </tr>
  </tbody>
</table>

<div class="remarks">
    <b>Remarks</b>
    <p>
        <li>
        Here we will focus on The Deterministic and Fully observable games.
        </li>
        <li>
            We have perfect information of the games.
        </li>
        <li>
            Can construct a tree that contains all possible outcomes because everything is fully determined.
        </li>
        <li>
            The main approach is to use Minimax: The computer is trying to minimize the possible maximum lost. Namely, depending on the moves of human players which is used to maximize the oppotunity for them to win, the computer is trying to minimize the lost from human players.
        </li>
    </p>
</div>
<p><br /></p>
<div class="remarks">
    <b>Remarks</b>
    <p>
        Since the tree is growing exponentiually, we don't actually need to explore every single possible node. We can selectively build a tree aysmmetrically while the main effort is used to explore the state that has more possibility to win.
    </p>
</div>

<h1 id="mcts-outline">MCTS Outline</h1>
<ul>
  <li>Descend through the tree</li>
  <li>Create new node</li>
  <li>Simulate</li>
  <li>Update the tree</li>
  <li>When we are out of time, return the “best” child node</li>
</ul>

<h1 id="for-a-tree-node-k">For a tree node $k$</h1>
<ul>
  <li>\(n_k\)  is the number of games played involving \(k\)</li>
  <li>\(w_k\) is the number of games won involving \(k\)</li>
</ul>

<h1 id="detailed-outline">Detailed Outline</h1>
<h2 id="descending">Descending</h2>
<p>Solution: <em>Upper Confidence Bound</em></p>

\[\begin{align*}
    UCB1(k,p) &amp;= E[win|k,p] + C \sqrt{\frac{2ln(n_{parent(k)})}{n_k}}\\
    &amp;\approx \frac{w_{k,p}}{n_k} + C \sqrt{\frac{2ln(n_{parent(k)})}{n_k}}
\end{align*}\]

<p>where the \(E[win|k,p]\) is the expand part and \(C \sqrt{\frac{2ln(n_{parent(k)})}{n_k}}\) is the explore part.
At each step, maximize \(UCB1(k,p)\)</p>

<h2 id="expanding">Expanding</h2>
<p>Make a new node and set \(n_k=0, w_k=0\)</p>

<h2 id="simulating">Simulating</h2>
<p>Randomly play the game, randomly chooose state nodes from all descending children. If the game is win, \(\Delta=+1\). If we lose or tie, \(\Delta =0\). But in most literature the results of descending children in simulation is not stored.</p>

<h2 id="updating-the-tree">Updating the Tree</h2>
<p>Propagate recursively up the parents.</p>

<p>Give simulation result $\Delta$,
for each \(k\):
    \(n_{k_new} = n_{k_old} + 1\)
    \(w_{k,1-new} = w_{k,1-old} + \Delta\)</p>

<h2 id="terminate">Terminate</h2>
<p>Return the best-ranked first ancestor!</p>

<h1 id="pros-and-cons">Pros and Cons</h1>
<h2 id="pros">Pros</h2>
<ul>
  <li>Grows tree asymmetrically, balancing expansion and exploration</li>
  <li>Depends only on the rules</li>
  <li>Easy to adapt to new games</li>
  <li>Heuristics not requried, but can also be integrated</li>
  <li>Can finish on demand, CPU time is proportional to answer quality</li>
  <li>Complete: guaranteed to find a solution give time</li>
  <li>Trivially parallelizable</li>
</ul>

<h2 id="cons">Cons</h2>
<ul>
  <li>Can’t handle extreme tree depth</li>
  <li>Requires ease of simulation, massive computation resources</li>
  <li>Relies on random play being “weakly correlated”</li>
  <li>Many variants, need expertise to tune</li>
  <li>Theoretical properties not yet understood</li>
</ul>]]></content><author><name>Duo Zhang</name></author><category term="media" /><summary type="html"><![CDATA[Motivation of Monte Carlo Tree Search (MCTS) Play games optimally To be able to play in a reasonalbe time Type of games   Deterministic Nondeterministic Fully Observable Chess, Checkers, Go Backgammon, Monopoly Partially Observable BattleShip Card Games Remarks Here we will focus on The Deterministic and Fully observable games. We have perfect information of the games. Can construct a tree that contains all possible outcomes because everything is fully determined. The main approach is to use Minimax: The computer is trying to minimize the possible maximum lost. Namely, depending on the moves of human players which is used to maximize the oppotunity for them to win, the computer is trying to minimize the lost from human players. Remarks Since the tree is growing exponentiually, we don't actually need to explore every single possible node. We can selectively build a tree aysmmetrically while the main effort is used to explore the state that has more possibility to win. MCTS Outline Descend through the tree Create new node Simulate Update the tree When we are out of time, return the “best” child node For a tree node $k$ \(n_k\) is the number of games played involving \(k\) \(w_k\) is the number of games won involving \(k\) Detailed Outline Descending Solution: Upper Confidence Bound \[\begin{align*} UCB1(k,p) &amp;= E[win|k,p] + C \sqrt{\frac{2ln(n_{parent(k)})}{n_k}}\\ &amp;\approx \frac{w_{k,p}}{n_k} + C \sqrt{\frac{2ln(n_{parent(k)})}{n_k}} \end{align*}\] where the \(E[win|k,p]\) is the expand part and \(C \sqrt{\frac{2ln(n_{parent(k)})}{n_k}}\) is the explore part. At each step, maximize \(UCB1(k,p)\) Expanding Make a new node and set \(n_k=0, w_k=0\) Simulating Randomly play the game, randomly chooose state nodes from all descending children. If the game is win, \(\Delta=+1\). If we lose or tie, \(\Delta =0\). But in most literature the results of descending children in simulation is not stored. Updating the Tree Propagate recursively up the parents. Give simulation result $\Delta$, for each \(k\): \(n_{k_new} = n_{k_old} + 1\) \(w_{k,1-new} = w_{k,1-old} + \Delta\) Terminate Return the best-ranked first ancestor! Pros and Cons Pros Grows tree asymmetrically, balancing expansion and exploration Depends only on the rules Easy to adapt to new games Heuristics not requried, but can also be integrated Can finish on demand, CPU time is proportional to answer quality Complete: guaranteed to find a solution give time Trivially parallelizable Cons Can’t handle extreme tree depth Requires ease of simulation, massive computation resources Relies on random play being “weakly correlated” Many variants, need expertise to tune Theoretical properties not yet understood]]></summary></entry><entry><title type="html">Vector Spaces and Linearity</title><link href="http://localhost:4000/Advanced_Linear_Algebra1_1/" rel="alternate" type="text/html" title="Vector Spaces and Linearity" /><published>2023-02-23T00:00:00-05:00</published><updated>2023-02-23T00:00:00-05:00</updated><id>http://localhost:4000/Advanced_Linear_Algebra1_1</id><content type="html" xml:base="http://localhost:4000/Advanced_Linear_Algebra1_1/"><![CDATA[<h1 id="algebraic-structures">Algebraic structures</h1>
<p>To start, we need some basic information of algebraic structure:</p>
<div class="definition">
<b>Definition</b>
<p>
    <b><i>Group:</i></b>
    A <b><mark class="red">group</mark></b> is a set \(\textit{G}\)  and associative binary opration * with:

    <ul>
        <li><b>closure:</b> \(\textit{a,b} \in \textit{G}\) implies \(\textit{a} * \textit{b} \in \textit{G}\)</li>
        <li><b>identity:</b> there exists \(\textit{e} \in \textit{G}\) such that \(\textit{a}*\textit{e} = \textit{e}*\textit{a} =\textit{a}\) for all \( \textit{a}\in \textit{G}\);</li>
        <li><b>inverses:</b> for all \(\textit{a} \in \textit{G}\), there is \(\textit{b} \in \textit{G}\) such that \(\textit{a}*\textit{b}=\textit{e}\)</li>
    </ul>
A group is <mark class="red"><b>abelian</b></mark> if \(\textit{a}*\textit{b}=\textit{b}*\textit{a}\) for all \(\textit{a},\textit{b} \in \textit{G}\)
</p>
</div>
<p><br /></p>
<div class="definition">
    <b>Definition</b>
    <p>
        <b><i>Field:</i></b>
        A <mark class="red"><b>field</b></mark> is a set \(\mathbb{F}\) or \(\textit{K}\) containing \(1 \neq 0\) with two binary operations: \(+\) and \(*\)(multiplication) such that:
        <ol>
            <li>\(\mathbb{F}\) is an abelian group under addition;</li>
            <li>\(\mathbb{F}\setminus \{0\}\) is an abelian group under multiplication;</li>
        </ol>
    </p>
</div>
<p><br /></p>
<div class="remarks">
    <b>Remarks</b>
    <p>
        <ol>
            <li>\(\mathbb{Q},\mathbb{R},\mathbb{C},\mathbb{Z}_p\)(prime \(p\)), \(\mathbb{Q}(\sqrt{2}):=\{a+b \sqrt{2}:a,b \in \mathbb{Q}\}\) are all fields</li>
            <li>\(\mathbb{Z}\) is not a field. Nor is \(\mathbb{Z}_n\)(composite n)</li>
            <li>The <i>additive identity</i> is 0, and the inverse of \(a\) is \(-a\)</li>
            <li>the <i>multiplicative identity</i> is 1, and the inverse of \(a\) is \(a^{-1}\), or \(\frac{1}{a}\)</li>
        </ol>
    </p>
</div>
<h1 id="vector-spaces">Vector Spaces</h1>
<p>Now we can define vector spaces:</p>
<div class="definition">
<b>Definition</b>
    <p>
        <b><i>Vector Space:</i></b>
        A <mark class="red"><b>vector space</b></mark> is a set \(X\)("vectors") over a filed \(\mathbb{F}\)("scalars") such that:
        <ol>
            <li>\(X\) is an <mark class="blue">abelian group</mark> under addition;</li>
            <li>\(X\) is closed under <mark class="blue">scalar multiplication</mark>;</li>
            <li>+ and * are "compatible" via natural <mark class="blue">associative</mark> and <mark class="blue">distributive</mark> laws relating the two:</li>
            $$
            \begin{align*}
                &amp; a(b \mathbf{v})=(ab)\mathbf{v}, &amp;&amp;\text{for all }a,b\in \mathbb{F}, \mathbf{v} \in X;\\
                &amp; a(\mathbf{v}+\mathbf{w})=a\mathbf{v}+a\mathbf{w}, &amp; &amp;\text{for all }a\in \mathbb{F}, \mathbf{v},\mathbf{w} \in X;\\
                &amp;(a+b)\mathbf{v}=a\mathbf{v}+b\mathbf{v},&amp; &amp;\text{for all }a,b\in \mathbb{F}, \textbf{v},\textbf{w}\in X;\\
                &amp;1 \textbf{v}=\textbf{v},&amp; &amp;\text{for all } \textbf{v}\in X
            \end{align*} 
            $$ 
        </ol>
        
    </p>
</div>
<p><br /></p>
<div class="random">
<b>Intuition</b>
    <p>
        Think of a vector space as a <mark class="blue">set of vectors</mark> that is:
        <ol>
            <li>
                Closed under addition, subtraction, and scalar multiplication;
            </li>
            <li>Equipped with the "natural" associative and distributive laws</li>
        </ol>
        
    </p>
</div>
<p><br /></p>
<div class="remarks">
    <b>Remarks</b>
    <p>
        In any vector space \(X\),
        <ol>
            <li>The zero vector \(\textbf{0}\) is unique</li>
            <p>
                <b>Proof:</b>
                Suppose the zero vector \(\textbf{0}\) is not unique, then we can assume there are two different zero vectors \(\textbf{0}_1, \textbf{0}_2\) in \(X\) (\(\textbf{0}_1\neq \textbf{0}_2\)). By the <mark class="red">identity</mark> property of a group, we have \(\textbf{0}_1 + \textbf{0}_2=\textbf{0}_1\) and \(\textbf{0}_1+ \textbf{0}_2=\textbf{0}_2\) which leads to a contradiction(\(\textbf{0}_1=\textbf{0}_2\))
            </p> 
            <li>\(0 \textbf{x}=\textbf{x}\) for all \(\textbf{x}\in X\)</li>
            <p>
                <b>Proof:</b>
                Suppose there's an \(\textbf{x}\) satisfy \(0 \textbf{x}\neq \textbf{0}\), thus we can assume that there is a vector \(\textbf{a}\neq 0\) satisfies \(0 \textbf{x}=\textbf{a}\). Thus we have for any scalar \(c\neq 0\): $$
                \begin{align*}
                    c \textbf{v} &amp;= (c+0) \textbf{v}\\
                    &amp;=c \textbf{v}+ 0 \textbf{v}\\
                    &amp;=c \textbf{v}+\textbf{a} 
                \end{align*}
                $$

                If the equation \(c \textbf{v}=c \textbf{v}+ \textbf{a}\) holds, the vector \(\textbf{a}\) must be a zero vector other than \(0\), which leads to the contradiction that there can be only one zero vector in the vector space.
            </p>        
            <li>\(-1 \textbf{x}=-\textbf{x}\) for all \(\textbf{x}\in X\)</li>
            <b>Proof:</b>
            $$
            \begin{align*}
                -1 \textbf{x}+x&amp;=-1 \textbf{x}+ 1 \textbf{x}\\
                &amp;=(-1+1) \textbf{x}\\   
                &amp;=0 \textbf{x}\\
                &amp;=\textbf{0}
            \end{align*}
            $$
            which means \(-1 \textbf{x}\) is the inverse of \(\textbf{x}\). Thus we can conclude that \(-1 \textbf{x}=\textbf{x} \) 
        </ol>
    </p>
</div>
<p><br /></p>
<div class="definition">
<b>Definition</b>
    <p>
        <b><i>Linear Map:</i></b>
        A <mark class="red"><b>linear map</b></mark> between vector spaces \(X\) and \(Y\) over \(\mathbb{F}\) is a function \(\varphi: X \rightarrow Y\) satisfying:
        <ul>
            <li>\(\varphi(\textbf{v}+\textbf{w})=\varphi(\textbf{v})+\varphi(\textbf{w})\) <span>&nbsp;</span> for all \(\textbf{v,w}\in X\)</li>
            <li>\(\varphi(a \textbf{v})=a\varphi (\textbf{v})\)<span>&nbsp;</span> for all \(a \in \mathbb{F}\)</li>
        </ul>
    </p>
    An <mark class="red">isomorphism</mark> is a linear map that is bijective.
</div>
<p><br /></p>
<div class="proposition">
<b>Propsition</b>
    <p>
        The two conditions for linearity above can be replaced by a single condition:
        \(\varphi(a \textbf{v}+b \textbf{w})=a\varphi(\textbf{v})+b\varphi(\textbf{w})\), for all \(\textbf{v,w}\in X \text{ and } a,b \in \mathbb{F}\)
    </p>
</div>
<p><br /></p>
<div class="random">
<b>Examples of vector spaces</b>
    <p>
        <ol>
            <li>
                \(K^n=\{(a_1,\cdots,a_n):a_i \in K\}\) with \(K\) is a field. Addition and multiplication are defined compomentwise.
            </li>
            <li>
                Set of functions \(\mathbb{R}\rightarrow \mathbb{R}\) with \(K=\mathbb{R}\)
            </li>
            <li>
                Set of functions \(S \rightarrow K\) for an abitrary set \(S\).
            </li>
            <li>
                Set of polynomials of degree \(&lt; n\), with coefficients from \(K\).
            </li>
        </ol>
        
    </p>
</div>
<p><br /></p>
<div class="remarks">
    <b>Remarks</b>
    <p>
        In the list of the vector spaces above, #1 is isomorphic to #4, and to #3 if \(|S|=n\).
        <p>
            <b>Proof:</b>
            <p>
            The vector space of #4 can be represented as \(P=a_0+a_1 x +\cdots+a_{n-1}x^{n-1}\) where \(a_i  \in K, i = 0,\cdots,n-1\). So we can consider the mapping \(\varphi: K^n\rightarrow K[x]\) which is a bijective mapping. We can actually assume that this is not a bijective mapping, then two different vectors from \(K\): \(\textbf{a}=(a_1,\cdots,a_n)\), \(\textbf{a}'=(a'_1,\cdots,a'_n)\) can result to the same polynomial \(P=p_0+p_1 x+\cdots+p_{n-1}x^{n-1}\).Thus we can know that the polynomial basis \(1,x,\cdots,x^{n-1}\) is reducible which leads to contradiction. This #1 is isomorphic to #4.
            </p>
            <p>
                For
            </p>
        </p>
    </p>
</div>]]></content><author><name>Duo Zhang</name></author><category term="media" /><summary type="html"><![CDATA[Algebraic structures To start, we need some basic information of algebraic structure: Definition Group: A group is a set \(\textit{G}\) and associative binary opration * with: closure: \(\textit{a,b} \in \textit{G}\) implies \(\textit{a} * \textit{b} \in \textit{G}\) identity: there exists \(\textit{e} \in \textit{G}\) such that \(\textit{a}*\textit{e} = \textit{e}*\textit{a} =\textit{a}\) for all \( \textit{a}\in \textit{G}\); inverses: for all \(\textit{a} \in \textit{G}\), there is \(\textit{b} \in \textit{G}\) such that \(\textit{a}*\textit{b}=\textit{e}\) A group is abelian if \(\textit{a}*\textit{b}=\textit{b}*\textit{a}\) for all \(\textit{a},\textit{b} \in \textit{G}\) Definition Field: A field is a set \(\mathbb{F}\) or \(\textit{K}\) containing \(1 \neq 0\) with two binary operations: \(+\) and \(*\)(multiplication) such that: \(\mathbb{F}\) is an abelian group under addition; \(\mathbb{F}\setminus \{0\}\) is an abelian group under multiplication; Remarks \(\mathbb{Q},\mathbb{R},\mathbb{C},\mathbb{Z}_p\)(prime \(p\)), \(\mathbb{Q}(\sqrt{2}):=\{a+b \sqrt{2}:a,b \in \mathbb{Q}\}\) are all fields \(\mathbb{Z}\) is not a field. Nor is \(\mathbb{Z}_n\)(composite n) The additive identity is 0, and the inverse of \(a\) is \(-a\) the multiplicative identity is 1, and the inverse of \(a\) is \(a^{-1}\), or \(\frac{1}{a}\) Vector Spaces Now we can define vector spaces: Definition Vector Space: A vector space is a set \(X\)("vectors") over a filed \(\mathbb{F}\)("scalars") such that: \(X\) is an abelian group under addition; \(X\) is closed under scalar multiplication; + and * are "compatible" via natural associative and distributive laws relating the two: $$ \begin{align*} &amp; a(b \mathbf{v})=(ab)\mathbf{v}, &amp;&amp;\text{for all }a,b\in \mathbb{F}, \mathbf{v} \in X;\\ &amp; a(\mathbf{v}+\mathbf{w})=a\mathbf{v}+a\mathbf{w}, &amp; &amp;\text{for all }a\in \mathbb{F}, \mathbf{v},\mathbf{w} \in X;\\ &amp;(a+b)\mathbf{v}=a\mathbf{v}+b\mathbf{v},&amp; &amp;\text{for all }a,b\in \mathbb{F}, \textbf{v},\textbf{w}\in X;\\ &amp;1 \textbf{v}=\textbf{v},&amp; &amp;\text{for all } \textbf{v}\in X \end{align*} $$ Intuition Think of a vector space as a set of vectors that is: Closed under addition, subtraction, and scalar multiplication; Equipped with the "natural" associative and distributive laws Remarks In any vector space \(X\), The zero vector \(\textbf{0}\) is unique Proof: Suppose the zero vector \(\textbf{0}\) is not unique, then we can assume there are two different zero vectors \(\textbf{0}_1, \textbf{0}_2\) in \(X\) (\(\textbf{0}_1\neq \textbf{0}_2\)). By the identity property of a group, we have \(\textbf{0}_1 + \textbf{0}_2=\textbf{0}_1\) and \(\textbf{0}_1+ \textbf{0}_2=\textbf{0}_2\) which leads to a contradiction(\(\textbf{0}_1=\textbf{0}_2\)) \(0 \textbf{x}=\textbf{x}\) for all \(\textbf{x}\in X\) Proof: Suppose there's an \(\textbf{x}\) satisfy \(0 \textbf{x}\neq \textbf{0}\), thus we can assume that there is a vector \(\textbf{a}\neq 0\) satisfies \(0 \textbf{x}=\textbf{a}\). Thus we have for any scalar \(c\neq 0\): $$ \begin{align*} c \textbf{v} &amp;= (c+0) \textbf{v}\\ &amp;=c \textbf{v}+ 0 \textbf{v}\\ &amp;=c \textbf{v}+\textbf{a} \end{align*} $$ If the equation \(c \textbf{v}=c \textbf{v}+ \textbf{a}\) holds, the vector \(\textbf{a}\) must be a zero vector other than \(0\), which leads to the contradiction that there can be only one zero vector in the vector space. \(-1 \textbf{x}=-\textbf{x}\) for all \(\textbf{x}\in X\) Proof: $$ \begin{align*} -1 \textbf{x}+x&amp;=-1 \textbf{x}+ 1 \textbf{x}\\ &amp;=(-1+1) \textbf{x}\\ &amp;=0 \textbf{x}\\ &amp;=\textbf{0} \end{align*} $$ which means \(-1 \textbf{x}\) is the inverse of \(\textbf{x}\). Thus we can conclude that \(-1 \textbf{x}=\textbf{x} \) Definition Linear Map: A linear map between vector spaces \(X\) and \(Y\) over \(\mathbb{F}\) is a function \(\varphi: X \rightarrow Y\) satisfying: \(\varphi(\textbf{v}+\textbf{w})=\varphi(\textbf{v})+\varphi(\textbf{w})\) &nbsp; for all \(\textbf{v,w}\in X\) \(\varphi(a \textbf{v})=a\varphi (\textbf{v})\)&nbsp; for all \(a \in \mathbb{F}\) An isomorphism is a linear map that is bijective. Propsition The two conditions for linearity above can be replaced by a single condition: \(\varphi(a \textbf{v}+b \textbf{w})=a\varphi(\textbf{v})+b\varphi(\textbf{w})\), for all \(\textbf{v,w}\in X \text{ and } a,b \in \mathbb{F}\) Examples of vector spaces \(K^n=\{(a_1,\cdots,a_n):a_i \in K\}\) with \(K\) is a field. Addition and multiplication are defined compomentwise. Set of functions \(\mathbb{R}\rightarrow \mathbb{R}\) with \(K=\mathbb{R}\) Set of functions \(S \rightarrow K\) for an abitrary set \(S\). Set of polynomials of degree \(&lt; n\), with coefficients from \(K\). Remarks In the list of the vector spaces above, #1 is isomorphic to #4, and to #3 if \(|S|=n\). Proof: The vector space of #4 can be represented as \(P=a_0+a_1 x +\cdots+a_{n-1}x^{n-1}\) where \(a_i \in K, i = 0,\cdots,n-1\). So we can consider the mapping \(\varphi: K^n\rightarrow K[x]\) which is a bijective mapping. We can actually assume that this is not a bijective mapping, then two different vectors from \(K\): \(\textbf{a}=(a_1,\cdots,a_n)\), \(\textbf{a}'=(a'_1,\cdots,a'_n)\) can result to the same polynomial \(P=p_0+p_1 x+\cdots+p_{n-1}x^{n-1}\).Thus we can know that the polynomial basis \(1,x,\cdots,x^{n-1}\) is reducible which leads to contradiction. This #1 is isomorphic to #4. For]]></summary></entry><entry><title type="html">How to set up Mujoco and Polyfempy on Greene and Singularity</title><link href="http://localhost:4000/How_to_set_up_mujoco_polyfempy_on_greene/" rel="alternate" type="text/html" title="How to set up Mujoco and Polyfempy on Greene and Singularity" /><published>2022-06-03T00:00:00-04:00</published><updated>2022-06-03T00:00:00-04:00</updated><id>http://localhost:4000/How_to_set_up_mujoco_polyfempy_on_greene</id><content type="html" xml:base="http://localhost:4000/How_to_set_up_mujoco_polyfempy_on_greene/"><![CDATA[<h1 id="set-up-mujoco-and-mujoco_py-on-greene">Set up Mujoco and Mujoco_py on Greene</h1>
<h2 id="approach-one">Approach One</h2>
<p>To set up mujoco on Greene HPC(NYU), the original tutorial is no longer working from <a href="https://www.notion.so/Setting-up-Mujoco-and-MPI-on-Greene-0fad2a6ac9e54115960f57a69e6ba6dd">Original Tutorial</a>. The basic steps are still the same, but there are a few minor changes.</p>
<ul>
  <li>Steps
    <ol>
      <li>Set up you own Singularity image and overlay according to <a href="https://github.com/nyu-dl/cluster-support/tree/master/greene">Singularity Tutorial</a>.</li>
      <li>Download mujoco200 or mujoco210(Newest version now) linux and unzip it inside ~/.mujoco</li>
      <li>If you are using mujoco200, please set go to <a href="https://www.roboti.us/license.html">Mjkey</a> and click on Activation key and download it. Then put the key to the bin subdirectory of your MuJoCo installation(e.g. ~/.mujoco/mujoco200/bin). If you are using mujoco210, then there’s no need to download the mjkey file. Just use it.</li>
      <li>copy
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> /scratch/work/public/singularity/mujoco200-dep-cuda10.1-ubuntu18.04-20201207.sqf 
</code></pre></div>        </div>
        <p>to</p>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> /scratch/$USER/&lt;workdir&gt;
</code></pre></div>        </div>
        <p>Or you can use any mujoco<em>.sqf files in folder <code class="language-plaintext highlighter-rouge">/scratch/work/public/singularity/</code> as long as it matches the cuda version you choose and the OS version in your singularity.
 Note that there’s no more any mujoco</em>.ext3 file system but only *sqf (Squashed file system) file. You can use these *.sqf file equally as *.ext3 file.</p>
      </li>
      <li>Now you can set up your singularity.sh file</li>
    </ol>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">        <span class="c">#!/bin/bash</span>

        <span class="c"># https://stackoverflow.com/questions/1668649/how-to-keep-quotes-in-bash-arguments</span>
        <span class="nv">args</span><span class="o">=</span><span class="s1">''</span>
        <span class="k">for </span>i <span class="k">in</span> <span class="s2">"</span><span class="nv">$@</span><span class="s2">"</span><span class="p">;</span> <span class="k">do
            </span><span class="nv">i</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">i</span><span class="p">//\\/\\\\</span><span class="k">}</span><span class="s2">"</span>
            <span class="nv">args</span><span class="o">=</span><span class="s2">"</span><span class="nv">$args</span><span class="s2"> </span><span class="se">\"</span><span class="k">${</span><span class="nv">i</span><span class="p">//\</span><span class="s2">"/</span><span class="se">\\\"</span><span class="s2">}</span><span class="se">\"</span><span class="s2">"</span><span class="p">
        done

        if [ </span><span class="s2">"</span><span class="nv">$args</span><span class="s2">"</span><span class="p"> == </span><span class="s2">""</span><span class="p"> ]; then args=/bin/bash; fi

        module purge

        export PATH=/share/apps/singularity/bin</span>:<span class="nv">$PATH</span><span class="p">

        # file systems
        export SINGULARITY_BINDPATH=/mnt,/scratch
        if [ -d /state/partition1 ]; then
            export SINGULARITY_BINDPATH=</span><span class="nv">$SINGULARITY_BINDPATH</span><span class="p">,/state/partition1
        fi

        # SLURM related
        export SINGULARITY_BINDPATH=</span><span class="nv">$SINGULARITY_BINDPATH</span><span class="p">,/opt/slurm,/usr/lib64/libmunge.so.2.0.0,/usr/lib64/libmunge.so.2,/var/run/munge,/etc/passwd
        export SINGULARITYENV_PREPEND_PATH=/opt/slurm/bin

        if [ -d /opt/slurm/lib64 ]; then
            export SINGULARITY_CONTAINLIBS=</span><span class="si">$(</span><span class="nb">echo</span> /opt/slurm/lib64/libpmi<span class="k">*</span> | xargs | <span class="nb">sed</span> <span class="nt">-e</span> <span class="s1">'s/ /,/g'</span><span class="si">)</span><span class="p">
        fi

        if [[ </span><span class="si">$(</span><span class="nb">hostname</span> <span class="nt">-s</span><span class="si">)</span><span class="p"> =~ ^g ]]; then nv=</span><span class="s2">"--nv"</span><span class="p">; fi

        singularity exec </span><span class="nv">$nv</span><span class="p"> \
                --overlay /scratch/</span><span class="nv">$USER</span><span class="p">/&lt;workdir&gt;/&lt;your_overly_file_system&gt;.ext3</span>:ro<span class="p"> \
                --overlay /scratch/</span><span class="nv">$USER</span><span class="p">/&lt;workdir&gt;/mujoco&lt;your_sqf_file&gt;.sfq</span>:ro<span class="p"> \
                /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif \
                /bin/bash -c </span><span class="s2">"
        source /ext3/env.sh
        conda activate &lt;your environment&gt;
        </span><span class="nv">$args</span><span class="s2">
        "</span><span class="p">
    </span></code></pre></figure>

<h2 id="some-other-tips-on-greene">Some other tips on Greene</h2>
<p>If you want to upgrade CMake or GCC on you singularity, use conda to install them.</p>
<ol>
  <li>Install CMake <code class="language-plaintext highlighter-rouge">conda install -c anaconda cmake</code>(<a href="https://anaconda.org/anaconda/cmake">https://anaconda.org/anaconda/cmake</a>)</li>
  <li>Install GCC <code class="language-plaintext highlighter-rouge">conda install -c conda-forge gcc</code> (<a href="https://anaconda.org/conda-forge/gcc">https://anaconda.org/conda-forge/gcc</a>)</li>
</ol>

<h2 id="approach-two">Approach Two</h2>
<p>In approach one, we included overlay file system <code class="language-plaintext highlighter-rouge">--overlay /scratch/$USER/&lt;workdir&gt;/mujoco&lt;your_sqf_file&gt;.sfq:ro</code>. In this file system, an /ext3 folder, /ext3/miniconda3 folder and /ext3/env.sh have already existed because these are required to set up mujoco in Singluarity. However, since this file system is read only, you cannot install or creat new conda environments in <code class="language-plaintext highlighter-rouge">ext3/miniconda3/envs</code> because you have no permission to modify this folder. The only option is create your own ~/.condarc to set new <code class="language-plaintext highlighter-rouge">envs_dirs</code> and <code class="language-plaintext highlighter-rouge">pkgs_dirs</code>. In approach 2, we can build mujoco from scratch. Here’s how we do it:
We choose Singularity OS image: /scratch/work/public/singularity/cuda11.1.1-cudnn8-devel-ubuntu20.04.sif with cuda support:</p>
<ul>
  <li>Steps
    <ol>
      <li>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   cp -rp /scratch/work/public/overlay-fs-ext3/overlay-50GB-10M.ext3.gz .
   gunzip overlay-15GB-500K.ext3.gz
   mv overlay-15GB-500K.ext3 ipc.ext3 
</code></pre></div>        </div>

        <p>Create your own file system(In my case, I am also creating the file system for ipc, so I rename my file system to ipc.ext3)</p>
      </li>
      <li>
        <p>Launch Singularity interactivly, bind <code class="language-plaintext highlighter-rouge">/share/apps</code> in order to use shell script <code class="language-plaintext highlighter-rouge">/share/apps/utils/singularity-conda/setup-conda.bash</code></p>

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   singularity \
   exec --nv \
   --bind /share/apps \
   --bind /usr/share/glvnd/egl_vendor.d/50_mesa.json \
   --overlay &lt;path-to-working-dir&gt;/ipc.ext3 \
   /scratch/work/public/singularity/cuda11.1.1-cudnn8-devel-ubuntu20.04.sif \
   /bin/bash
</code></pre></div>        </div>

        <p>Here, <code class="language-plaintext highlighter-rouge">--bind /usr/share/glvnd/egl_vendor.d/50_mesa.json</code> is to include the <code class="language-plaintext highlighter-rouge">&lt;GL/osmesa.h&gt;</code> during installation of mujoco_py.</p>
      </li>
      <li>Get Mujoco210 and unzip it
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> cd /ext3
 wget https://github.com/deepmind/mujoco/releases/download/2.1.0/mujoco210-linux-x86_64.tar.gz
 tar -vxzf mujoco210-linux-x86_64.tar.gz
</code></pre></div>        </div>
      </li>
      <li>We also need a copy inside ~/.mujoco because some packages installtion will use the header files there
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> mkdir -p ~/.mujoco
 cd ~/.mujoco
 wget https://github.com/deepmind/mujoco/releases/download/2.1.0/mujoco210-linux-x86_64.tar.gz
 tar -vxzf mujoco210-linux-x86_64.tar.gz
</code></pre></div>        </div>
      </li>
      <li>To setup miniconda with wrapper script
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> cd /ext3
 bash /share/apps/utils/singularity-conda/setup-conda.bash 
</code></pre></div>        </div>
        <p>Now miniconda is ready.</p>
      </li>
      <li>Enable conda by <code class="language-plaintext highlighter-rouge">source /ext3/env.sh</code></li>
      <li>Create your own conda environment. In my case, I will be using the <code class="language-plaintext highlighter-rouge">environment.yml</code> from <a href="https://github.com/arvigj/ipc-robotics/tree/duo_edit">IPC-Robotics</a>.
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda env create -f environment.yml
</code></pre></div>        </div>
      </li>
      <li>Activate your environment <code class="language-plaintext highlighter-rouge">conda activate ipc-robotics</code></li>
      <li>Download mujoco-py 2.1.2.14 to anywhere you want and unzip it:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> wget https://github.com/openai/mujoco-py/archive/refs/tags/v2.1.2.14.tar.gz
 tar -vxzf v2.1.2.14.tar.gz
</code></pre></div>        </div>
      </li>
      <li>Modify the file mujoco-py-2.1.2.14/mujoco_py/builder.py and add these lines after line 30 for Singularity, current implementation only supports docker and host.
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>singularity_path = '/.singularity.d/libs'
if exists(singularity_path) :
    return singularity_path 
</code></pre></div>        </div>
        <p>which makes the original function look like:</p>
      </li>
    </ol>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python">            <span class="k">def</span> <span class="nf">get_nvidia_lib_dir</span><span class="p">():</span>
                <span class="n">exists_nvidia_smi</span> <span class="o">=</span> <span class="n">subprocess</span><span class="p">.</span><span class="n">call</span><span class="p">(</span><span class="s">"type nvidia-smi"</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                        <span class="n">stdout</span><span class="o">=</span><span class="n">subprocess</span><span class="p">.</span><span class="n">PIPE</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="p">.</span><span class="n">PIPE</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">exists_nvidia_smi</span><span class="p">:</span>
                    <span class="k">return</span> <span class="bp">None</span>
                
                <span class="n">singularity_path</span> <span class="o">=</span> <span class="s">'/.singularity.d/libs'</span>
                <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="n">singularity_path</span><span class="p">)</span> <span class="p">:</span>
                    <span class="k">return</span> <span class="n">singularity_path</span> 
            
                <span class="n">docker_path</span> <span class="o">=</span> <span class="s">'/usr/local/nvidia/lib64'</span>
                <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="n">docker_path</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">docker_path</span>

                <span class="n">nvidia_path</span> <span class="o">=</span> <span class="s">'/usr/lib/nvidia'</span>
                <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="n">nvidia_path</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">nvidia_path</span>

                <span class="n">paths</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="n">glob</span><span class="p">(</span><span class="s">'/usr/lib/nvidia-[0-9][0-9][0-9]'</span><span class="p">)</span>
                <span class="n">paths</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">return</span> <span class="bp">None</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">"Choosing the latest nvidia driver: %s, among %s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">paths</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="nb">str</span><span class="p">(</span><span class="n">paths</span><span class="p">)))</span>

                <span class="k">return</span> <span class="n">paths</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        </code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.  Then we can install mujoco_py
    ```
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/.mujoco/mujoco210/bin

    cd mujoco-py-2.1.2.14
    pip --no-cache-dir install . -I
    ```
10. Now we can create the env file for mujoco_py.
    ```
    touch /ext3/mujoco.sh
    echo "export MUJOCO_GL=\"egl\"" &gt;&gt; /ext3/mujoco.sh
    echo "export MJLIB_PATH=/ext3/mujoco210/bin/libmujoco210.so" &gt;&gt; /ext3/mujoco.sh
    ```
    Now every time you start your singluarity, you need to 
    ```
    source /ext3/env.sh
    source /ext3/mujoco.sh
    ```


Now Mujoco and Mujoco_py are set up on Greene.
</code></pre></div></div>

<h1 id="set-up-polyfempy-in-singularity">Set up Polyfempy in Singularity</h1>
<p>We still need to use Singularity OS image: /scratch/work/public/singularity/cuda11.1.1-cudnn8-devel-ubuntu20.04.sif with cuda support. Note that any other ubuntu image whose version is lower than 20.04 will have potential problems.</p>

<ul>
  <li>Steps:
    <ol>
      <li>Use the file system created above to start singularity interactivly:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> singularity \
 exec --nv \
 --bind /share/apps \
 --bind /usr/share/glvnd/egl_vendor.d/50_mesa.json \
 --overlay &lt;path-to-working-dir&gt;/ipc.ext3 \
 /scratch/work/public/singularity/cuda11.1.1-cudnn8-devel-ubuntu20.04.sif \
 /bin/bash
</code></pre></div>        </div>
      </li>
      <li>Enable conda
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> source /ext3/env.sh
</code></pre></div>        </div>
      </li>
      <li>Activate your environment
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda activate ipc-robotics
</code></pre></div>        </div>
      </li>
      <li>Use conda to install higher version of CMake
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda install -c anaconda cmake
</code></pre></div>        </div>
      </li>
      <li>Get polyfempy from source:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> git clone git@github.com:polyfem/polyfem-python.git
 cd polyfem-python
</code></pre></div>        </div>
      </li>
      <li>Install polyfempy from source
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> python setup.py install
</code></pre></div>        </div>
      </li>
      <li>Waiting for installation to finish</li>
      <li>We can do some tests:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> python
 &gt;&gt;&gt; import polyfempy as pf
 &gt;&gt;&gt; solver = pf.Solver()
 &gt;&gt;&gt; solver.set_log_level(3)
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ul>

<!-- ## MathJax

You can enable MathJax by setting `mathjax: true` on a page or globally in the `_config.yml`. Some examples:

[Euler's formula](https://en.wikipedia.org/wiki/Euler%27s_formula) relates the  complex exponential function to the trigonometric functions.

$$ e^{i\theta}=\cos(\theta)+i\sin(\theta) $$

The [Euler-Lagrange](https://en.wikipedia.org/wiki/Lagrangian_mechanics) differential equation is the fundamental equation of calculus of variations.

$$ \frac{\mathrm{d}}{\mathrm{d}t} \left ( \frac{\partial L}{\partial \dot{q}} \right ) = \frac{\partial L}{\partial q} $$

The [Schrödinger equation](https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation) describes how the quantum state of a quantum system changes with time.

$$ i\hbar\frac{\partial}{\partial t} \Psi(\mathbf{r},t) = \left [ \frac{-\hbar^2}{2\mu}\nabla^2 + V(\mathbf{r},t)\right ] \Psi(\mathbf{r},t) $$

## Code

Embed code by putting `{% highlight language %}` `{% endhighlight %}` blocks around it. Adding the parameter `linenos` will show source lines besides the code.


<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="k">static</span> <span class="kt">void</span> <span class="nf">asyncEnabled</span><span class="p">(</span><span class="n">Dict</span><span class="o">*</span> <span class="n">args</span><span class="p">,</span> <span class="kt">void</span><span class="o">*</span> <span class="n">vAdmin</span><span class="p">,</span> <span class="n">String</span><span class="o">*</span> <span class="n">txid</span><span class="p">,</span> <span class="k">struct</span> <span class="n">Allocator</span><span class="o">*</span> <span class="n">requestAlloc</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">struct</span> <span class="n">Admin</span><span class="o">*</span> <span class="n">admin</span> <span class="o">=</span> <span class="n">Identity_check</span><span class="p">((</span><span class="k">struct</span> <span class="n">Admin</span><span class="o">*</span><span class="p">)</span> <span class="n">vAdmin</span><span class="p">);</span>
    <span class="kt">int64_t</span> <span class="n">enabled</span> <span class="o">=</span> <span class="n">admin</span><span class="o">-&gt;</span><span class="n">asyncEnabled</span><span class="p">;</span>
    <span class="n">Dict</span> <span class="n">d</span> <span class="o">=</span> <span class="n">Dict_CONST</span><span class="p">(</span><span class="n">String_CONST</span><span class="p">(</span><span class="s">"asyncEnabled"</span><span class="p">),</span> <span class="n">Int_OBJ</span><span class="p">(</span><span class="n">enabled</span><span class="p">),</span> <span class="nb">NULL</span><span class="p">);</span>
    <span class="n">Admin_sendMessage</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d</span><span class="p">,</span> <span class="n">txid</span><span class="p">,</span> <span class="n">admin</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure>


## Gists

With the `jekyll-gist` plugin, which is preinstalled on Github Pages, you can embed gists simply by using the `gist` command:

<script src="https://gist.github.com/5555251.js?file=gist.md"></script>

## Images

Upload an image to the *assets* folder and embed it with `![title](/assets/name.jpg))`. Keep in mind that the path needs to be adjusted if Jekyll is run inside a subfolder.

A wrapper `div` with the class `large` can be used to increase the width of an image or iframe.

![Flower](https://user-images.githubusercontent.com/4943215/55412447-bcdb6c80-5567-11e9-8d12-b1e35fd5e50c.jpg)

[Flower](https://unsplash.com/photos/iGrsa9rL11o) by Tj Holowaychuk

## Embedded content

You can also embed a lot of stuff, for example from YouTube, using the `embed.html` include.

<div style="position: relative; margin: 1.5em 0; padding-bottom: 56.25%;">
  <iframe style="position: absolute;" src="https://www.youtube.com/embed/_C0A5zX-iqM" width="100%" height="100%" frameborder="0" allowfullscreen></iframe>
</div>
 -->
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://kraftoreo.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>]]></content><author><name>Duo Zhang</name></author><category term="media" /><summary type="html"><![CDATA[Set up Mujoco and Mujoco_py on Greene Approach One To set up mujoco on Greene HPC(NYU), the original tutorial is no longer working from Original Tutorial. The basic steps are still the same, but there are a few minor changes. Steps Set up you own Singularity image and overlay according to Singularity Tutorial. Download mujoco200 or mujoco210(Newest version now) linux and unzip it inside ~/.mujoco If you are using mujoco200, please set go to Mjkey and click on Activation key and download it. Then put the key to the bin subdirectory of your MuJoCo installation(e.g. ~/.mujoco/mujoco200/bin). If you are using mujoco210, then there’s no need to download the mjkey file. Just use it. copy /scratch/work/public/singularity/mujoco200-dep-cuda10.1-ubuntu18.04-20201207.sqf to /scratch/$USER/&lt;workdir&gt; Or you can use any mujoco.sqf files in folder /scratch/work/public/singularity/ as long as it matches the cuda version you choose and the OS version in your singularity. Note that there’s no more any mujoco.ext3 file system but only *sqf (Squashed file system) file. You can use these *.sqf file equally as *.ext3 file. Now you can set up your singularity.sh file #!/bin/bash # https://stackoverflow.com/questions/1668649/how-to-keep-quotes-in-bash-arguments args='' for i in "$@"; do i="${i//\\/\\\\}" args="$args \"${i//\"/\\\"}\"" done if [ "$args" == "" ]; then args=/bin/bash; fi module purge export PATH=/share/apps/singularity/bin:$PATH # file systems export SINGULARITY_BINDPATH=/mnt,/scratch if [ -d /state/partition1 ]; then export SINGULARITY_BINDPATH=$SINGULARITY_BINDPATH,/state/partition1 fi # SLURM related export SINGULARITY_BINDPATH=$SINGULARITY_BINDPATH,/opt/slurm,/usr/lib64/libmunge.so.2.0.0,/usr/lib64/libmunge.so.2,/var/run/munge,/etc/passwd export SINGULARITYENV_PREPEND_PATH=/opt/slurm/bin if [ -d /opt/slurm/lib64 ]; then export SINGULARITY_CONTAINLIBS=$(echo /opt/slurm/lib64/libpmi* | xargs | sed -e 's/ /,/g') fi if [[ $(hostname -s) =~ ^g ]]; then nv="--nv"; fi singularity exec $nv \ --overlay /scratch/$USER/&lt;workdir&gt;/&lt;your_overly_file_system&gt;.ext3:ro \ --overlay /scratch/$USER/&lt;workdir&gt;/mujoco&lt;your_sqf_file&gt;.sfq:ro \ /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif \ /bin/bash -c " source /ext3/env.sh conda activate &lt;your environment&gt; $args " Some other tips on Greene If you want to upgrade CMake or GCC on you singularity, use conda to install them. Install CMake conda install -c anaconda cmake(https://anaconda.org/anaconda/cmake) Install GCC conda install -c conda-forge gcc (https://anaconda.org/conda-forge/gcc) Approach Two In approach one, we included overlay file system --overlay /scratch/$USER/&lt;workdir&gt;/mujoco&lt;your_sqf_file&gt;.sfq:ro. In this file system, an /ext3 folder, /ext3/miniconda3 folder and /ext3/env.sh have already existed because these are required to set up mujoco in Singluarity. However, since this file system is read only, you cannot install or creat new conda environments in ext3/miniconda3/envs because you have no permission to modify this folder. The only option is create your own ~/.condarc to set new envs_dirs and pkgs_dirs. In approach 2, we can build mujoco from scratch. Here’s how we do it: We choose Singularity OS image: /scratch/work/public/singularity/cuda11.1.1-cudnn8-devel-ubuntu20.04.sif with cuda support: Steps cp -rp /scratch/work/public/overlay-fs-ext3/overlay-50GB-10M.ext3.gz . gunzip overlay-15GB-500K.ext3.gz mv overlay-15GB-500K.ext3 ipc.ext3 Create your own file system(In my case, I am also creating the file system for ipc, so I rename my file system to ipc.ext3) Launch Singularity interactivly, bind /share/apps in order to use shell script /share/apps/utils/singularity-conda/setup-conda.bash singularity \ exec --nv \ --bind /share/apps \ --bind /usr/share/glvnd/egl_vendor.d/50_mesa.json \ --overlay &lt;path-to-working-dir&gt;/ipc.ext3 \ /scratch/work/public/singularity/cuda11.1.1-cudnn8-devel-ubuntu20.04.sif \ /bin/bash Here, --bind /usr/share/glvnd/egl_vendor.d/50_mesa.json is to include the &lt;GL/osmesa.h&gt; during installation of mujoco_py. Get Mujoco210 and unzip it cd /ext3 wget https://github.com/deepmind/mujoco/releases/download/2.1.0/mujoco210-linux-x86_64.tar.gz tar -vxzf mujoco210-linux-x86_64.tar.gz We also need a copy inside ~/.mujoco because some packages installtion will use the header files there mkdir -p ~/.mujoco cd ~/.mujoco wget https://github.com/deepmind/mujoco/releases/download/2.1.0/mujoco210-linux-x86_64.tar.gz tar -vxzf mujoco210-linux-x86_64.tar.gz To setup miniconda with wrapper script cd /ext3 bash /share/apps/utils/singularity-conda/setup-conda.bash Now miniconda is ready. Enable conda by source /ext3/env.sh Create your own conda environment. In my case, I will be using the environment.yml from IPC-Robotics. conda env create -f environment.yml Activate your environment conda activate ipc-robotics Download mujoco-py 2.1.2.14 to anywhere you want and unzip it: wget https://github.com/openai/mujoco-py/archive/refs/tags/v2.1.2.14.tar.gz tar -vxzf v2.1.2.14.tar.gz Modify the file mujoco-py-2.1.2.14/mujoco_py/builder.py and add these lines after line 30 for Singularity, current implementation only supports docker and host. singularity_path = '/.singularity.d/libs' if exists(singularity_path) : return singularity_path which makes the original function look like: def get_nvidia_lib_dir(): exists_nvidia_smi = subprocess.call("type nvidia-smi", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) == 0 if not exists_nvidia_smi: return None singularity_path = '/.singularity.d/libs' if exists(singularity_path) : return singularity_path docker_path = '/usr/local/nvidia/lib64' if exists(docker_path): return docker_path nvidia_path = '/usr/lib/nvidia' if exists(nvidia_path): return nvidia_path paths = glob.glob('/usr/lib/nvidia-[0-9][0-9][0-9]') paths = sorted(paths) if len(paths) == 0: return None if len(paths) &gt; 1: print("Choosing the latest nvidia driver: %s, among %s" % (paths[-1], str(paths))) return paths[-1] 9. Then we can install mujoco_py ``` export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/.mujoco/mujoco210/bin cd mujoco-py-2.1.2.14 pip --no-cache-dir install . -I ``` 10. Now we can create the env file for mujoco_py. ``` touch /ext3/mujoco.sh echo "export MUJOCO_GL=\"egl\"" &gt;&gt; /ext3/mujoco.sh echo "export MJLIB_PATH=/ext3/mujoco210/bin/libmujoco210.so" &gt;&gt; /ext3/mujoco.sh ``` Now every time you start your singluarity, you need to ``` source /ext3/env.sh source /ext3/mujoco.sh ```]]></summary></entry><entry><title type="html">Transfering Simulated Learning to Real Robots</title><link href="http://localhost:4000/Sim2Real(RL-class-project)/" rel="alternate" type="text/html" title="Transfering Simulated Learning to Real Robots" /><published>2021-12-16T20:31:19-05:00</published><updated>2021-12-16T20:31:19-05:00</updated><id>http://localhost:4000/Sim2Real(RL%20class%20project)</id><content type="html" xml:base="http://localhost:4000/Sim2Real(RL-class-project)/"><![CDATA[<p>Lately, Robotics has become a major playfield for Reinforcement Learning. Learning-based algorithms have the potential to enable robots to acquire complex behaviors adaptively in unstructured environments by leveraging data collected from the environment. In particular, with reinforcement learning, robots learn novel behaviors through trial and error interactions. This unburdens the human operator from having to pre-program accurate behaviors. This is particularly important as we deploy robots in scenarios where the environment may not be known.</p>

<p>However, not only is training in the real world very time-consuming, it also puts the robots and the environment at considerable risk. To evade these problems, training in simulation and then transferring to the real is becoming a popular strategy. Training in simulation can lead to substantial speed-ups because of the possibility of parallelization without any significant extra costs.</p>

<p>But these models trained in simulation suffer from what is termed as the Reality Gap. This reality gap might arise because of multiple reasons such as omitting some physical phenomena, having inaccurate parameterized values of physical quantities or even small numerical approximations in typical solvers. One can argue that ideally all of these could be solved by building more accurate simulators, but that’s easier said than done. Moreover, the real world itself is non-stationary!</p>

<p>Our project deals with exploring methods to reduce this sim2real gap and deploy a model trained to do the Reach task in simulation on a real Xarm7 robot. Our goal is to deploy the model in the real world, with nothing but images as the current state input.</p>

<h1 id="training-setup">Training Setup</h1>
<p>We use Mujoco for simulating our environment and depicted the real robot setup in terms of color and camera position as closely as possible.
Training in simulaiton provides many advatages. The most important amongst those is access to state variables instead of just the images. This allows us to speed up training by using <a href="https://arxiv.org/abs/1710.06542">Asymmetric Actor Critic</a> model.
The Asymmetric Actor Critic uses a <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html">DDPG</a> based approach, where the Actor gets image-based inputs but the Critic at the time of training gets state-based input.</p>

<h3 id="feature-extractor">Feature Extractor</h3>
<p>We use a 3-layer Conv network to extract the current state (a 3D coordinate of the end effector’s position) from the Current state image (84x84x3) input. It is this state estimation using the feature extractor that is passed to the Actor in Test and Training time. Besides the RL loss that is backpropagated by the Actor Network, we also add an additional loss called the Bottleneck loss on the feature extractor to speed up training. The Bottleneck loss minimizes the difference between the actual state (which we get at the time of training) and the predicted loss by the feature extractor.</p>

<p>The loss function for the feature extractor looks something like-</p>

\[L_f = L_{RL}  + MSE(s,s_{predicted})\]

<p>The Bottleneck loss also led to significant speed gains in training the feature extractor.</p>

<h3 id="actor-network">Actor Network</h3>
<p>The Actor takes in the predicted current state output from the feature extractor and the goal as a 3D coordinate as input, and outputs a 3D vector equivalent of the displacement in the 3D space.
The actor optimizes on the loss-</p>

<p>\(L_a = -E_s[Q(s,\pi(s))]\)
Where \(Q(s,\pi(s))\) is the estimated Q-value by the Critic.
During deployment we only use the output of the actor.</p>

<h3 id="critic-network">Critic Network</h3>
<p>The Critic takes in the action yielded by the Actor along with the current state as a 3D coordinate of the end effector and the goal as a 3D coordinate as input. It outputs the predicted Q-value of the action given the current state and goal as a single float value.
The critic tries to optimize over the following loss function-</p>

\[L_c = ( Q(s_t,a_t) - r_t - \gamma Q_T(s_{t+1},\pi(s_{t+1}) )  )^2\]

<p>Here the \(Q_T\) is predicted using the Target networks (Actor and Critic) which are updated using Polyak averaging of the weights of the corresponding current Actor and Critic networks.</p>

<center><img src="/assets/files/sim2real.png" width="75%" /></center>

<h1 id="adapting-to-the-real-world">Adapting to the Real World</h1>
<p>Here, we discuss the optimizations we made during training and deployment time to bridge the sim2real gap.</p>

<h3 id="getting-past-the-curse-of-depth">Getting Past the Curse of Depth</h3>
<p>Sometimes it is hard to gather Depth perception from plain images which leads to performance dips in the model in simulation and in the real world. In an ideal scenario, we would have liked to have additional information on depth. Depth is easy to gather in simulation. The real world on the hand was a different case. We tried using the Depth output of the Intel Realsense cameras, but much to our dismay it lacked the consistency and accuracy required for our model to work properly.</p>

<p>Here’s the juxtaposition of the depth information yielded by the simulation and the real world-</p>

<center><img src="/assets/files/depth.png" width="596" /></center>
<center> Depth image from simulation </center>

<center><iframe width="596" height="336" src="https://www.youtube.com/embed/FYNW7Zaa-lo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></center>
<center> Depth image from RealSense2 Camera </center>
<p>Looking at these results we decided move away from depth information altogether.</p>

<p>Fortunately we noticed that using a side camera angle allowed a more reasonable perception of depth. This is probably because the model has parameters such as the extension of the arm to latch onto to better estimate depth.</p>

<center><img src="/assets/files/still_image.png" width="596" /></center>
<center> The side view of the XArm </center>

<h3 id="domain-randomisation">Domain Randomisation</h3>
<p>The idea of Domain Randomization is based on the idea of generalisation. There are certain attributes of the simulation that might not perfectly mimic the real world leading to the sim2real gap. Randomising these attributes during training should develop resilience in the model to the corresponding variations in the real world.</p>

<p>If handled cavalierly, Domain Randomisation could lead to an unreasonable increase in training time or in worst case scenarios not train at all.</p>

<p>After a lot of trails, we settled on the following forms of Domain Randomisations for every episode-</p>
<ol>
  <li>Camera angle variation: Here we vary the camera angle slightly to compensate for slight discrepancy in camera positioning in the real world.</li>
  <li>Lighting variation: Here we randomise the lighting in the simulation. This leads to resilience in the reflective and slight color variations in the real world.</li>
</ol>

<center><iframe width="596" height="448" src="https://www.youtube.com/embed/5LpJUl7Ee3E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></center>
<center> The video showing domain randomization </center>

<h1 id="expriments-with-different-setups">Expriments with different setups</h1>
<h3 id="best-results--fully-rl-based--bottleneck--initialized-feature-extractor-no-depth-information">Best Results- Fully RL-based + Bottleneck + Initialized Feature extractor, no depth information</h3>
<p>Since the depth information is too messy in real world to provide a good estimation of distance for the robot, we decided not to use the depth channel from the camera RealSense2. Trained</p>

<center><iframe width="596" height="336" src="https://www.youtube.com/embed/SuhteFGvjHk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></center>
<center>Reach Task using full RL-based + bottleneck model </center>
<p>From this video we can see that the performance of the robot is pretty good, and the final gestures are really good mimics of what is shown in the simulation located at the bottom left corner. However, due to the size of the input images(\(84\times 84\)) differs from the frame size captured by RealSense2 Camera, sometimes the robot gripper can get out of the frame range of RealSense2 Camera and the model no longer knows where the gripper is. Thus when the target is generated really low and close to the came, this reach task will fail. Also, when the whole body of the robot vertically faces the camera frame, the model loses its precision for estimating the distance information and the configuration of the robot, which inevitably leads to failure. Nonetheless, if a new target is generated in a “good area”(good area means the area where the camera captures the side view of the robot almost all the time and the gripper will not get out the camera frame.), the model can surprisingly guide the robot gripper leave the blind area and go to the new target.</p>

<h3 id="fully-rl-based-no-bottleneck-no-depth-no-initialized-feature-extractor">Fully RL-based, No Bottleneck, No Depth, No Initialized Feature Extractor</h3>
<p>In this scenerio, both the view of angle and the position of the camera are changed to test the ability of the model trained by our domain randomization strategy to generalize among different views.</p>
<center><iframe width="596" height="336" src="https://www.youtube.com/embed/8auil7amjDI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </center>
<center>Reach Task using full RL-based model with variation in camera position </center>
<p>From the video we know that although the results are not that good compared with our best model, it still shows some capability to optimize for different views which can compensate a lot for the errors when attempting to replicate the configurations in the simulator. However, this model shares the same problem with the best model–the gripper can get out of the camera frame and the model loses the distance information and the estimation of the robot configuration.</p>

<h3 id="rl-based--initialized-feature-extractor--depth-information-no-bottleneck">RL-based + Initialized Feature Extractor + Depth Information, No Bottleneck</h3>

<p>In this experiment, we want to know how much performance can be affected by the real world depth information. Here’s the result:</p>
<center> <iframe width="596" height="336" src="https://www.youtube.com/embed/xiiS1zGqjCQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></center>
<center>Reach Task using RL-based model with pretrained feature extractor and depth information </center>
<p>During the training time, with the boost of pre-trained feature extractor, the performance of the model in the training time is very good, and converges faster than the models which don’t have pre-trained feature extractor. However, when the model is transferred into real world, the depth information sabotages the performance of the model. As demonstrated in the video, when the gripper reaches into the overlay area of the corner of the curtain and the gripper from the view angle of the camera, despite a perfect side view in our eyes, the model completely lost its conception of the world and starts to do some random moves with no underlying logic.</p>

<h1 id="training-time-performance">Training time performance</h1>

<p>In our experiments, we can show that the performance of the model (our best model + depth information) actually does better than our best model. Sadly the chaotic depth image from RealSense2 camera ruined everything. We also tried to use AlexNet as the feature extractor which is shown in the dark green curve. From this we know that the pre-trained AlexNet can only do better at the beginning of the training, and in later stages, its performance is climbing slowly and it takes more time to train if we want to achieve similar performance with a 3-layer CNN feature extractor.</p>
<center><img src="/assets/files/bottleneck_depth_vs_non_depth.png" width="596" /></center>
<center> Training time performance of different models </center>

<h1 id="future-works">Future works</h1>
<ol>
  <li>Our ultimate goal is to train the robot to do Pick &amp; Place task. However, this model takes days to train and we never managed to transfer the model to real world though it is doing a great job in the simulator.</li>
  <li>We want to use more cameras to feed the feature extractor with multiple images from different angles so that the model can get a better estimation of the distance and the configuration of the robot without getting depth information involved and that also the model won’t be befuddled and bewildered when the robot is directly facing the camera frame and the whole body is almost vertical to the plane of the camera frame.</li>
</ol>

<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://kraftoreo.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>]]></content><author><name>Duo Zhang</name></author><summary type="html"><![CDATA[Lately, Robotics has become a major playfield for Reinforcement Learning. Learning-based algorithms have the potential to enable robots to acquire complex behaviors adaptively in unstructured environments by leveraging data collected from the environment. In particular, with reinforcement learning, robots learn novel behaviors through trial and error interactions. This unburdens the human operator from having to pre-program accurate behaviors. This is particularly important as we deploy robots in scenarios where the environment may not be known. However, not only is training in the real world very time-consuming, it also puts the robots and the environment at considerable risk. To evade these problems, training in simulation and then transferring to the real is becoming a popular strategy. Training in simulation can lead to substantial speed-ups because of the possibility of parallelization without any significant extra costs. But these models trained in simulation suffer from what is termed as the Reality Gap. This reality gap might arise because of multiple reasons such as omitting some physical phenomena, having inaccurate parameterized values of physical quantities or even small numerical approximations in typical solvers. One can argue that ideally all of these could be solved by building more accurate simulators, but that’s easier said than done. Moreover, the real world itself is non-stationary! Our project deals with exploring methods to reduce this sim2real gap and deploy a model trained to do the Reach task in simulation on a real Xarm7 robot. Our goal is to deploy the model in the real world, with nothing but images as the current state input. Training Setup We use Mujoco for simulating our environment and depicted the real robot setup in terms of color and camera position as closely as possible. Training in simulaiton provides many advatages. The most important amongst those is access to state variables instead of just the images. This allows us to speed up training by using Asymmetric Actor Critic model. The Asymmetric Actor Critic uses a DDPG based approach, where the Actor gets image-based inputs but the Critic at the time of training gets state-based input.]]></summary></entry><entry><title type="html">Advanced examples</title><link href="http://localhost:4000/examples/" rel="alternate" type="text/html" title="Advanced examples" /><published>2017-01-01T00:00:00-05:00</published><updated>2017-01-01T00:00:00-05:00</updated><id>http://localhost:4000/examples</id><content type="html" xml:base="http://localhost:4000/examples/"><![CDATA[<p><img src="https://user-images.githubusercontent.com/4943215/55412536-edbba180-5567-11e9-9c70-6d33bca3f8ed.jpg" alt="Swiss Alps" /></p>

<h2 id="mathjax">MathJax</h2>

<p>You can enable MathJax by setting <code class="language-plaintext highlighter-rouge">mathjax: true</code> on a page or globally in the <code class="language-plaintext highlighter-rouge">_config.yml</code>. Some examples:</p>

<p><a href="https://en.wikipedia.org/wiki/Euler%27s_formula">Euler’s formula</a> relates the  complex exponential function to the trigonometric functions.</p>

\[e^{i\theta}=\cos(\theta)+i\sin(\theta)\]

<p>The <a href="https://en.wikipedia.org/wiki/Lagrangian_mechanics">Euler-Lagrange</a> differential equation is the fundamental equation of calculus of variations.</p>

\[\frac{\mathrm{d}}{\mathrm{d}t} \left ( \frac{\partial L}{\partial \dot{q}} \right ) = \frac{\partial L}{\partial q}\]

<p>The <a href="https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation">Schrödinger equation</a> describes how the quantum state of a quantum system changes with time.</p>

\[i\hbar\frac{\partial}{\partial t} \Psi(\mathbf{r},t) = \left [ \frac{-\hbar^2}{2\mu}\nabla^2 + V(\mathbf{r},t)\right ] \Psi(\mathbf{r},t)\]

<h2 id="code">Code</h2>

<p>Embed code by putting <code class="language-plaintext highlighter-rouge">{% highlight language %}</code> <code class="language-plaintext highlighter-rouge">{% endhighlight %}</code> blocks around it. Adding the parameter <code class="language-plaintext highlighter-rouge">linenos</code> will show source lines besides the code.</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="k">static</span> <span class="kt">void</span> <span class="nf">asyncEnabled</span><span class="p">(</span><span class="n">Dict</span><span class="o">*</span> <span class="n">args</span><span class="p">,</span> <span class="kt">void</span><span class="o">*</span> <span class="n">vAdmin</span><span class="p">,</span> <span class="n">String</span><span class="o">*</span> <span class="n">txid</span><span class="p">,</span> <span class="k">struct</span> <span class="n">Allocator</span><span class="o">*</span> <span class="n">requestAlloc</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">struct</span> <span class="n">Admin</span><span class="o">*</span> <span class="n">admin</span> <span class="o">=</span> <span class="n">Identity_check</span><span class="p">((</span><span class="k">struct</span> <span class="n">Admin</span><span class="o">*</span><span class="p">)</span> <span class="n">vAdmin</span><span class="p">);</span>
    <span class="kt">int64_t</span> <span class="n">enabled</span> <span class="o">=</span> <span class="n">admin</span><span class="o">-&gt;</span><span class="n">asyncEnabled</span><span class="p">;</span>
    <span class="n">Dict</span> <span class="n">d</span> <span class="o">=</span> <span class="n">Dict_CONST</span><span class="p">(</span><span class="n">String_CONST</span><span class="p">(</span><span class="s">"asyncEnabled"</span><span class="p">),</span> <span class="n">Int_OBJ</span><span class="p">(</span><span class="n">enabled</span><span class="p">),</span> <span class="nb">NULL</span><span class="p">);</span>
    <span class="n">Admin_sendMessage</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d</span><span class="p">,</span> <span class="n">txid</span><span class="p">,</span> <span class="n">admin</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure>

<h2 id="gists">Gists</h2>

<p>With the <code class="language-plaintext highlighter-rouge">jekyll-gist</code> plugin, which is preinstalled on Github Pages, you can embed gists simply by using the <code class="language-plaintext highlighter-rouge">gist</code> command:</p>

<script src="https://gist.github.com/5555251.js?file=gist.md"></script>

<h2 id="images">Images</h2>

<p>Upload an image to the <em>assets</em> folder and embed it with <code class="language-plaintext highlighter-rouge">![title](/assets/name.jpg))</code>. Keep in mind that the path needs to be adjusted if Jekyll is run inside a subfolder.</p>

<p>A wrapper <code class="language-plaintext highlighter-rouge">div</code> with the class <code class="language-plaintext highlighter-rouge">large</code> can be used to increase the width of an image or iframe.</p>

<p><img src="https://user-images.githubusercontent.com/4943215/55412447-bcdb6c80-5567-11e9-8d12-b1e35fd5e50c.jpg" alt="Flower" /></p>

<p><a href="https://unsplash.com/photos/iGrsa9rL11o">Flower</a> by Tj Holowaychuk</p>

<h2 id="embedded-content">Embedded content</h2>

<p>You can also embed a lot of stuff, for example from YouTube, using the <code class="language-plaintext highlighter-rouge">embed.html</code> include.</p>

<div style="position: relative; margin: 1.5em 0; padding-bottom: 56.25%;">
  <iframe style="position: absolute;" src="https://www.youtube.com/embed/_C0A5zX-iqM" width="100%" height="100%" frameborder="0" allowfullscreen=""></iframe>
</div>

<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://kraftoreo.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>]]></content><author><name>Duo Zhang</name></author><category term="media" /><summary type="html"><![CDATA[]]></summary></entry></feed>